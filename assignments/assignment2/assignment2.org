#+TITLE: ECE 420 Assignment 2
#+AUTHOR: Arun Woosaree
#+LaTeX_CLASS: article
#+LATEX_CLASS_OPTIONS: [letterpaper]
#+latex_header: \usepackage{amsthm}
#+latex_header: \newtheorem{thm}{}
#+OPTIONS: toc:nil
#+begin_src elisp :exports none
(setq org-latex-listings 'minted
      org-latex-packages-alist '(("" "minted"))
      org-latex-minted-options '(("linenos" "true"))
      org-latex-pdf-process
      '("pdflatex -shell-escape -interaction nonstopmode -output-directory %o %f"
        "pdflatex -shell-escape -interaction nonstopmode -output-directory %o %f"))
#+end_src

#+RESULTS:
| pdflatex -shell-escape -interaction nonstopmode -output-directory %o %f | pdflatex -shell-escape -interaction nonstopmode -output-directory %o %f |

* Trapezoidal Rule

** static
~schedule(static, 2)~ means that OpenMP will divide the iterations into chunks of size 2, and the chunks are distributed to the threads in round robin fashion.
So the distribution will look something like this:

#+begin_example
thread0: 1 2 5 6 9  10 13 14 ...
thread1: 3 4 7 8 11 12 15 16 ...
#+end_example
** guided
~schedule(guided)~ means that OpenMP will divide the iterations into chunks, and each thread executes a chunk of iterations and requests another chunk until no more chunks are available. The default chunk size is 1. The chunk size decreases each time a chunk of work is given to a thread. The initial chunk size is proportional to num iterations / num threads, while subsequent chunks are proportional to the remaining number of iterations / num threads. The iterations will look like this:


| Thread | Iterations | Sizeof Chunk  | Remaining Iterations |
|--------+------------+---------------+----------------------|
|      0 |     1-5000 | 5000          |                 4999 |
|      1 |  5001-7500 | 4999/2 = 2500 |                 2499 |
|      1 |  7501-8750 | 2499/2 = 1250 |                 1249 |
|      1 |  8751-9375 | 1249/2 = 625  |                  624 |
|      1 |  9376-9687 | 624/2 = 312   |                  312 |
|      1 |  9688-9843 | 312/2 = 156   |                  156 |
|      1 |  9844-9921 | 156/2 = 78    |                   78 |
|      1 |  9922-9960 | 78/2 = 39     |                   39 |
|      1 |  9961-9980 | 39/2 = 20     |                   19 |
|      1 |  9981-9990 | 20/2 = 10     |                    9 |
|      1 |  9991-9995 | 10/2 = 5      |                    4 |
|      1 |  9996-9997 | 5/2 = 2       |                    2 |
|      1 |       9998 | 2/2 = 1       |                    1 |
|      1 |       9999 | 1             |                    0 |

Because we're ignoring scheduling/synchronization work assignment overhead, thread 1 finishes its smaller batches of work before thread 0 all the time. This results in thread 0 doing iterations 1-5000, while thread 1 does iterations 5001-9999.

* Odd-Even Transposition Sort
#+begin_src c
for (phase = 0; phase < n; phase++) {
  if (phase % 2 == 0)
#pragma omp parallel for numthreads(threadcount) default(none)                 \
    shared(a, n) private(i, tmp)
    for (i = 1; i < n; i += 2) {
      if (a[i - 1] > a[i]) {
        tmp = a[i - 1];
        a[i - 1] = a[i];
        a[i] = tmp;
      }
    }
  else
#pragma omp parallel for numthreads(threadcount) default(none)                 \
    shared(a, n) private(i, tmp)
    for (i = 1; i < n - 1; i += 2) {
      if (a[i] > a[i + 1]) {
        tmp = a[i + 1];
        a[i + 1] = a[i];
        a[i] = tmp;
      }
    }
}
#+end_src
This is inefficient because there is an implicit ~fork~ and ~join~ for each ~parallel for~ iteration.
This can be fixed by instead replacing each ~parallel for~ with just for.

So, lines 3-4, and lines 13-14 above would both be replaced by this:
#+begin_src c
#pragma omp for
#+end_src

There is an example table in the notes which shows the improvement in run time:

| thread count              |     1 |     2 |     3 |     4 |
|---------------------------+-------+-------+-------+-------|
| 2 parallel for directives | 0.770 | 0.453 | 0.358 | 0.305 |
| 2 for directives          | 0.732 | 0.376 | 0.294 | 0.239 |

* Maximum Value

#+begin_src c
int largest = 0;
#pragma omp parallel for
for (int i = 0; i < 1000; i++) {
#pragma omp critical
  if (data[i] > largest)
    largest = data[i];
}
#+end_src

We can improve the performance of this code by first computing a local maximum for each section that each thread works on.
That way, we hit the critical section less often. We do this by using the openmp directive to say that lp is a private
local variable for each thread. When each thread finds its local maximum, the local maxumum is compared to the
global maximum, and if it is higher, the global maximum is updated:

#+begin_src c
int largest = 0;
int lp

#pragma omp parallel private(lp)
{
  lp = 0;
#pragma omp for
  for (int i = 0; i < 1000; i++) {
    if (data[i] > lp)
      lp = data[i];
  }
  if (lp > largest) {
#pragma critical
    {
      if (lp > largest)
        largest = lp;
    }
  }
}
#+end_src

* Matrix Vector Multiplication
#+begin_src c
// Parallelize by rows
#pragma omp parallel default(none) shared(v2, v1, matrix, tam) private(i, j)
{
#pragma omp for
  for (i = 0; i < tam; i++)
    for (j = 0; j < tam; j++)
      v2[i] += matrix[i][j] * v1[j];
}
#+end_src
The first example, parallelize by rows is fine.
#+begin_src c
// Parallelize by columns
#pragma omp parallel default(none) shared(j, v2, v1, matrix, tam) private(i, j)
{
  for (i = 0; i < tam; i++)
#pragma omp for
    for (j = 0; j < tam; j++)
      v2[i] += matrix[i][j] * v1[j];
}
#+end_src
The second example needs some work. First, we see that the variable ~j~
is declared as both a shared and a private variable. Furthermore, ~i~ is declared as a shared variable, when we are parallelizing by columns. To fix this, each thread can do a local calculation of what would go into ~v2[i]~, then update the global variable.
Alternatively, the OpenMP reduction directive can be used.

#+begin_src c
// Parallelize by columns
#pragma omp parallel default(none) shared(i, v2, v1, matrix, tam) private(j)
{
  for (i = 0; i < tam; i++)
#pragma omp for reduction(+ : v2[i])
    for (j = 0; j < tam; j++)
      v2[i] += matrix[i][j] * v1[j];
}
#+end_src

* Output of Program

Here is an explanation of what the code does: (found on docs.oracle.com)
#+begin_src c
#include <omp.h>
#include <stdio.h>
int main() {
  omp_set_nested(1);
  omp_set_dynamic(0);
#pragma omp parallel num_threads(2)
  {
    if (omp_get_thread_num() == 0)
      omp_set_num_threads(4); /* line A */
    else
      omp_set_num_threads(6); /* line B */

    /* The following statement will print out
     *
     * 0: 2 4
     * 1: 2 6
     *
     * omp_get_num_threads() returns the number
     * of the threads in the team, so it is
     * the same for the two threads in the team.
     */
    printf("%d: %d %d\n", omp_get_thread_num(), omp_get_num_threads(),
           omp_get_max_threads());

/* Two inner parallel regions will be created
 * one with a team of 4 threads, and the other
 * with a team of 6 threads.
 */
#pragma omp parallel
    {
#pragma omp master
      {
        /* The following statement will print out
         *
         * Inner: 4
         * Inner: 6
         */
        printf("Inner: %d\n", omp_get_num_threads());
      }
      omp_set_num_threads(7); /* line C */
    }

    /* Again two inner parallel regions will be created,
     * one with a team of 4 threads, and the other
     * with a team of 6 threads.
     *
     * The omp_set_num_threads(7) call at line C
     * has no effect here, since it affects only
     * parallel regions at the same or inner nesting
     * level as line C.
     */

#pragma omp parallel
    { printf("count me.\n"); }
  }
  return (0);
}
#+end_src

One possible output is:
#+begin_example
0: 2 4
Inner: 4
1: 2 6
Inner: 6
count me.
count me.
count me.
count me.
count me.
count me.
count me.
count me.
count me.
count me.
#+end_example

I also got the following outputs by compiling and running the program on my computer:
#+begin_example
0: 2 4
1: 2 6
Inner: 4
count me.
count me.
count me.
count me.
Inner: 6
count me.
count me.
count me.
count me.
count me.
count me.
#+end_example


#+begin_example
1: 2 6
0: 2 4
Inner: 6
Inner: 4
count me.
count me.
count me.
count me.
count me.
count me.
count me.
count me.
count me.
count me.
#+end_example

Of course, there are more possible outputs which can be found by running the program more times.


* Fibonacci
#+begin_example
export OMP_NUM_THREADS = 3
export OMP_NESTED = TRUE
export OMP_DYNAMIC = FALSE
#+end_example

** Parallelize using OpenMP Sections Directives
#+begin_src c
#include <omp.h>
#include <stdio.h>

int fib(int n) {
  int i, j;
  if (n < 2)
    return n;
  else {
#pragma omp parallel sections
    {
#pragma omp section
      { i = fib(n - 1); }
#pragma omp section
      { j = fib(n - 2); }
    }
    return i + j;
  }
}

int main() {
  int result;
  int n = 10;

  result = fib(n);
  printf("Result is %d\n", result);
  return 0;
}
#+end_src
** Parallelize using OpenMP Tasks Directives
#+begin_src c
#include <omp.h>
#include <stdio.h>
int fib(int n) {
  int i, j;
  if (n < 2)
    return n;
  else {
#pragma omp task shared(i) firstprivate(n)
    i = fib(n - 1);

#pragma omp task shared(j) firstprivate(n)
    j = fib(n - 2);

#pragma omp taskwait
    return i + j;
  }
}

int main() {
  int result;
  int n = 5;

#pragma omp parallel shared(n)
  {
#pragma omp single
    {
      result = fib(n);
      printf("Result is %d\n", result);
    }
  }
  return 0;
}
#+end_src

*** num threads in 1)
After compiling and running the sections code above, it seems that 21 threads are ever launched

*** num threads in 2)
After compiling and running the tasks code above, it seems that there are three threads that are ever launched.
