#+TITLE: ECE 420 Assignment 2
#+AUTHOR: Arun Woosaree
#+LaTeX_CLASS: article
#+LATEX_CLASS_OPTIONS: [letterpaper]
#+latex_header: \usepackage{amsthm}
#+latex_header: \newtheorem{thm}{}
#+OPTIONS: toc:nil
#+begin_src elisp :exports none
(setq org-latex-listings 'minted
      org-latex-packages-alist '(("" "minted"))
      org-latex-minted-options '(("linenos" "true"))
      org-latex-pdf-process
      '("pdflatex -shell-escape -interaction nonstopmode -output-directory %o %f"
        "pdflatex -shell-escape -interaction nonstopmode -output-directory %o %f"))
#+end_src

#+RESULTS:
| pdflatex -shell-escape -interaction nonstopmode -output-directory %o %f | pdflatex -shell-escape -interaction nonstopmode -output-directory %o %f |

* Trapezoidal Rule

** static
~schedule(static, 2)~ means that OpenMP will divide the iterations into chunks of size 2, and the chunks are distributed to the threads in round robin fashion.
So thread 0 wil get iterations 1, 3, 5, ... 9999 while thread 1 will get iterations 2, 4, 6, ... 9998.
** guided
~schedule(guided)~ means that OpenMP will divide the iterations into chunks, and each thread executes a chunk of iterations and requests another chunk until no more chunks are available. The default chunk size is 1. The chunk size decreases each time a chunk of work is given to a thread. The initial chunk size is proportional to num iterations / num threads, while subsequent chunks are proportional to the remaining number of iterations / num threads. The iterations will look like this:


| Thread |     Chunk | Sizeof Chunk  | Remaining Iterations |
|--------+-----------+---------------+----------------------|
|      0 |    1-5000 | 5000          |                 4999 |
|      1 | 5001-7500 | 4999/2 = 2500 |                 2499 |
|      1 | 7501-8750 | 2499/2 = 1250 |                 1249 |
|      1 | 8751-9375 | 1249/2 = 625  |                  624 |
|      0 | 9376-9687 | 624/2 = 312   |                  312 |
|      1 | 9688-9843 | 312/2 = 156   |                  156 |
|      0 | 9844-9921 | 156/2 = 78    |                   78 |
|      1 | 9922-9960 | 78/2 = 39     |                   39 |
|      1 | 9961-9980 | 39/2 = 20     |                   19 |
|      1 | 9981-9990 | 20/2 = 10     |                    9 |
|      1 | 9991-9995 | 10/2 = 5      |                    4 |
|      0 | 9996-9997 | 5/2 = 2       |                    2 |
|      1 |      9998 | 2/2 = 1       |                    1 |
|      0 |      9999 | 1             |                    0 |



* Odd-Even Transposition Sort
#+begin_src c
for (phase = 0; phase < n; phase++) {
  if (phase % 2 == 0)
#pragma omp parallel for numthreads(threadcount) default(none)                 \
    shared(a, n) private(i, tmp)
    for (i = 1; i < n; i += 2) {
      if (a[i - 1] > a[i]) {
        tmp = a[i - 1];
        a[i - 1] = a[i];
        a[i] = tmp;
      }
    }
  else
#pragma omp parallel for numthreads(threadcount) default(none)                 \
    shared(a, n) private(i, tmp)
    for (i = 1; i < n - 1; i += 2) {
      if (a[i] > a[i + 1]) {
        tmp = a[i + 1];
        a[i + 1] = a[i];
        a[i] = tmp;
      }
    }
}
#+end_src
This is inefficient because there is an implicit ~fork~ and ~join~ for each ~parallel for~ iteration.
This can be fixed by instead replacing each ~parallel for~ with just for.

So, lines 3-4, and lines 13-14 above would both be replaced by this:
#+begin_src c
#pragma omp for
#+end_src

There is an example table in the notes which shows the improvement in run time:

| thread count              |     1 |     2 |     3 |     4 |
|---------------------------+-------+-------+-------+-------|
| 2 parallel for directives | 0.770 | 0.453 | 0.358 | 0.305 |
| 2 for directives          | 0.732 | 0.376 | 0.294 | 0.239 |

* Maximum Value

* Matrix Vector Multiplication

* Output of Program

* Fibonacci
