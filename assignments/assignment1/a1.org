#+TITLE:ECE 420 Assignment 1
#+Author: Arun Woosaree

#+LaTeX_CLASS: article
#+LATEX_CLASS_OPTIONS: [letterpaper]
#+latex_header: \usepackage{amsthm}
#+latex_header: \newtheorem{thm}{}
#+OPTIONS: toc:nil

* Flynn's Taxonomy
** SISD
SISD stands for Single Instruction Stream, Single Data Stream. A SISD system has a single uniprocessor, which executes a single instruction stream to operate on data stored in a single.

An example that would fit in this category is a single core superscalar processor like the AMD 29050
Reference: https://en.wikipedia.org/wiki/Superscalar_processor

** SIMD
SIMD stands for Single Instruction Stream, Multiple Data Stream. A SIMD system has multiple processing units which all simultaneously do the same operation on multiple points in a data pool in a synchronized fashion.

An example that would fit in this category is a modern graphics processing unit, like the AMD Radeon 6900 XT

** MISD
MISD stands for Multiple Instruction Stream, Single Data Stream. A MISD system has multiple processing units performing different operations on the same data simultaneously. It can be used for reliability or fault tolerance, if the same operation is being done multiple times on the same data.

An example of an MISD system is the computer responsible for the Space Shuttle flight control. The Space Shuttle is also known as the low orbit spacecraft that helped launch the Hubble telescope, among other space missions.
References: https://en.wikipedia.org/wiki/MISD#cite_note-1 https://en.wikipedia.org/wiki/Space_Shuttle

** MIMD
MIMD stands for Multiple Instruction Stream, Multiple Data Stream. A MIMD system has many processing units which each perform operations independently from each other at the same time. That is, each unit can be doing different operations on different pieces of data at any given time.

An example of a MIMD system is almost any modern multicore processor, like the AMD Ryzen 9 5950X.
Another example could be a computer cluster, or a network of workstations.

* Shared Memory vs Distributed Memory
** Shared Memory
*** Pros
+ user friendly programming perspective to memory
+ data sharing between tasks is fast and uniform
*** Cons
- no scalability between memory and CPUs
- if you want more power,
** Distributed Memory
*** Pros
+ memory is scalable with the number of processors
+ this is cost effective, because lots of cheap, commodity hardware can be used as opposed to upgrading a monolithic, more expensive structure
*** Cons
- data communication between processors is more difficult
- it is difficult to map existing data structures to the memory organization
- memory access times can be very different. For example, local access times vs over a network
  access times can be much slower compared to a MIMD system
- more overhead for the programmer. The programmer has to think about message passing

* Amdahl's Law

\begin{thm}
    If \(y\) fraction of a serial program cannot be parallelized, \(1/y\) is an upper bound on the speedup of its parallel program, no matter how many processing elements are used.
\end{thm}

\begin{proof}
If \(y\) is the fraction of a serial program that cannot be parallelized, then the fraction \(x\) which is the fraction of the program that can be parallelized is found by:
\begin{equation}\label{oneminus}
x =  (1-y)
\end{equation}

According to Amdahl's law, the upper limit for speedup of a parallel program is:
\begin{equation}\label{upperlim}
\lim_{p \to \infty} S(p) \leq \frac{1}{1-x}
\end{equation}
where \(p\) is the number of processing elements.

Substituting equation \ref{oneminus} into \ref{upperlim}, we get:

\begin{equation}\label{almostthere}
\lim_{p \to \infty} S(p) \leq \frac{1}{1-(1-y)}
\end{equation}

Simplifying \ref{almostthere}, we get:

\begin{equation}
\lim_{p \to \infty} S(p) \leq \frac{1}{y}
\end{equation}

This proves that \(1/y\) is an upper bound for the speedup of a program, no matter how many processing elements are used, given that \(y\) is the fraction of a serial program that cannot be parallelized
\end{proof}
